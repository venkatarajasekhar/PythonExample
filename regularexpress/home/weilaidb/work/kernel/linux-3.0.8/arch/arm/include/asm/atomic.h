#define __ASM_ARM_ATOMIC_H
#define ATOMIC_INIT(i)
#define atomic_read(v)	(*(volatile int *)&(v)->counter)
#define atomic_set(v,i)	(((v)->counter) = (i))
#if __LINUX_ARM_ARCH__ >= 6
static inline void atomic_add(int i, atomic_t *v)
static inline int atomic_add_return(int i, atomic_t *v)
static inline void atomic_sub(int i, atomic_t *v)
static inline int atomic_sub_return(int i, atomic_t *v)
static inline int atomic_cmpxchg(atomic_t *ptr, int old, int new)
static inline void atomic_clear_mask(unsigned long mask, unsigned long *addr)
#error SMP not supported on pre-ARMv6 CPUs
static inline int atomic_add_return(int i, atomic_t *v)
#define atomic_add(i, v)	(void) atomic_add_return(i, v)
static inline int atomic_sub_return(int i, atomic_t *v)
#define atomic_sub(i, v)	(void) atomic_sub_return(i, v)
static inline int atomic_cmpxchg(atomic_t *v, int old, int new)
static inline void atomic_clear_mask(unsigned long mask, unsigned long *addr)
#define atomic_xchg(v, new) (xchg(&((v)->counter), new))
static inline int atomic_add_unless(atomic_t *v, int a, int u)
#define atomic_inc_not_zero(v) atomic_add_unless((v), 1, 0)
#define atomic_inc(v)		atomic_add(1, v)
#define atomic_dec(v)		atomic_sub(1, v)
#define atomic_inc_and_test(v)	(atomic_add_return(1, v) == 0)
#define atomic_dec_and_test(v)	(atomic_sub_return(1, v) == 0)
#define atomic_inc_return(v)    (atomic_add_return(1, v))
#define atomic_dec_return(v)    (atomic_sub_return(1, v))
#define atomic_sub_and_test(i, v) (atomic_sub_return(i, v) == 0)
#define atomic_add_negative(i,v) (atomic_add_return(i, v) < 0)
#define smp_mb__before_atomic_dec()	smp_mb()
#define smp_mb__after_atomic_dec()	smp_mb()
#define smp_mb__before_atomic_inc()	smp_mb()
#define smp_mb__after_atomic_inc()	smp_mb()
typedef struct  atomic64_t;
#define ATOMIC64_INIT(i)
static inline u64 atomic64_read(atomic64_t *v)
static inline void atomic64_set(atomic64_t *v, u64 i)
static inline void atomic64_add(u64 i, atomic64_t *v)
static inline u64 atomic64_add_return(u64 i, atomic64_t *v)
static inline void atomic64_sub(u64 i, atomic64_t *v)
static inline u64 atomic64_sub_return(u64 i, atomic64_t *v)
static inline u64 atomic64_cmpxchg(atomic64_t *ptr, u64 old, u64 new)
static inline u64 atomic64_xchg(atomic64_t *ptr, u64 new)
static inline u64 atomic64_dec_if_positive(atomic64_t *v)
static inline int atomic64_add_unless(atomic64_t *v, u64 a, u64 u)
#define atomic64_add_negative(a, v)	(atomic64_add_return((a), (v)) < 0)
#define atomic64_inc(v)			atomic64_add(1LL, (v))
#define atomic64_inc_return(v)		atomic64_add_return(1LL, (v))
#define atomic64_inc_and_test(v)	(atomic64_inc_return(v) == 0)
#define atomic64_sub_and_test(a, v)	(atomic64_sub_return((a), (v)) == 0)
#define atomic64_dec(v)			atomic64_sub(1LL, (v))
#define atomic64_dec_return(v)		atomic64_sub_return(1LL, (v))
#define atomic64_dec_and_test(v)	(atomic64_dec_return((v)) == 0)
#define atomic64_inc_not_zero(v)	atomic64_add_unless((v), 1LL, 0LL)
