#define AVUTIL_MIPS_GENERIC_MACROS_MSA_H
#define ALIGNMENT           16
ALLOC_ALIGNED __attribute__ ((aligned((align) << 1)))
LD_B *((RTYPE *)(psrc))
LD_UB LD_B(v16u8, __VA_ARGS__)
LD_SB LD_B(v16i8, __VA_ARGS__)
LD_H *((RTYPE *)(psrc))
LD_UH LD_H(v8u16, __VA_ARGS__)
LD_SH LD_H(v8i16, __VA_ARGS__)
LD_W *((RTYPE *)(psrc))
LD_UW LD_W(v4u32, __VA_ARGS__)
LD_SW LD_W(v4i32, __VA_ARGS__)
ST_B *((RTYPE *)(pdst)) = (in)
ST_UB ST_B(v16u8, __VA_ARGS__)
ST_SB ST_B(v16i8, __VA_ARGS__)
ST_H *((RTYPE *)(pdst)) = (in)
ST_UH ST_H(v8u16, __VA_ARGS__)
ST_SH ST_H(v8i16, __VA_ARGS__)
ST_W *((RTYPE *)(pdst)) = (in)
ST_UW ST_W(v4u32, __VA_ARGS__)
ST_SW ST_W(v4i32, __VA_ARGS__)
#if (__mips_isa_rev >= 6)
LW                           \
(  )
#if (__mips == 64)
LD                           \
(  )
LD                                              \
(  )
SH                      \
SW                      \
SD                      \
LW                           \
(  )
#if (__mips == 64)
LD                           \
(  )
LD                                              \
(  )
SH                      \
SW                      \
SD                                          \
LW4  \
LD2  \
LD4  \
SW4  \
SD4  \
LD_B2  \
LD_UB2 LD_B2(v16u8, __VA_ARGS__)
LD_SB2 LD_B2(v16i8, __VA_ARGS__)
LD_B3  \
LD_UB3 LD_B3(v16u8, __VA_ARGS__)
LD_SB3 LD_B3(v16i8, __VA_ARGS__)
LD_B4   \
LD_UB4 LD_B4(v16u8, __VA_ARGS__)
LD_SB4 LD_B4(v16i8, __VA_ARGS__)
LD_B5  \
LD_UB5 LD_B5(v16u8, __VA_ARGS__)
LD_SB5 LD_B5(v16i8, __VA_ARGS__)
LD_B6  \
LD_UB6 LD_B6(v16u8, __VA_ARGS__)
LD_SB6 LD_B6(v16i8, __VA_ARGS__)
LD_B7          \
LD_UB7 LD_B7(v16u8, __VA_ARGS__)
LD_SB7 LD_B7(v16i8, __VA_ARGS__)
LD_B8           \
LD_UB8 LD_B8(v16u8, __VA_ARGS__)
LD_SB8 LD_B8(v16i8, __VA_ARGS__)
LD_H2  \
LD_UH2 LD_H2(v8u16, __VA_ARGS__)
LD_SH2 LD_H2(v8i16, __VA_ARGS__)
LD_H4  \
LD_UH4 LD_H4(v8u16, __VA_ARGS__)
LD_SH4 LD_H4(v8i16, __VA_ARGS__)
LD_H6  \
LD_UH6 LD_H6(v8u16, __VA_ARGS__)
LD_SH6 LD_H6(v8i16, __VA_ARGS__)
LD_H8           \
LD_UH8 LD_H8(v8u16, __VA_ARGS__)
LD_SH8 LD_H8(v8i16, __VA_ARGS__)
LD_H16  \
LD_SH16 LD_H16(v8i16, __VA_ARGS__)
LD4x4_SH                \
LD_SW2  \
ST_B2  \
ST_UB2 ST_B2(v16u8, __VA_ARGS__)
ST_SB2 ST_B2(v16i8, __VA_ARGS__)
ST_B4    \
ST_UB4 ST_B4(v16u8, __VA_ARGS__)
ST_SB4 ST_B4(v16i8, __VA_ARGS__)
ST_B8                                         \
ST_UB8 ST_B8(v16u8, __VA_ARGS__)
ST_H2  \
ST_UH2 ST_H2(v8u16, __VA_ARGS__)
ST_SH2 ST_H2(v8i16, __VA_ARGS__)
ST_H4    \
ST_SH4 ST_H4(v8i16, __VA_ARGS__)
ST_H6  \
ST_SH6 ST_H6(v8i16, __VA_ARGS__)
ST_H8  \
ST_SH8 ST_H8(v8i16, __VA_ARGS__)
ST_SW2  \
ST_SW8                            \
ST2x4_UB              \
ST4x2_UB             \
ST4x4_UB  \
ST4x8_UB                            \
ST6x4_UB       \
ST8x1_UB                   \
ST8x2_UB             \
ST8x4_UB                      \
ST8x8_UB        \
ST12x4_UB                \
ST12x8_UB  \
AVER_UB2       \
AVER_UB2_UB AVER_UB2(v16u8, __VA_ARGS__)
AVER_UB4                        \
AVER_UB4_UB AVER_UB4(v16u8, __VA_ARGS__)
SLDI_B2_0                 \
SLDI_B2_0_UB SLDI_B2_0(v16u8, __VA_ARGS__)
SLDI_B2_0_SB SLDI_B2_0(v16i8, __VA_ARGS__)
SLDI_B2_0_SW SLDI_B2_0(v4i32, __VA_ARGS__)
SLDI_B3_0     \
SLDI_B3_0_UB SLDI_B3_0(v16u8, __VA_ARGS__)
SLDI_B3_0_SB SLDI_B3_0(v16i8, __VA_ARGS__)
SLDI_B4_0    \
SLDI_B4_0_UB SLDI_B4_0(v16u8, __VA_ARGS__)
SLDI_B4_0_SB SLDI_B4_0(v16i8, __VA_ARGS__)
SLDI_B4_0_SH SLDI_B4_0(v8i16, __VA_ARGS__)
SLDI_B2  \
SLDI_B2_UB SLDI_B2(v16u8, __VA_ARGS__)
SLDI_B2_SB SLDI_B2(v16i8, __VA_ARGS__)
SLDI_B2_SH SLDI_B2(v8i16, __VA_ARGS__)
SLDI_B3                               \
SLDI_B3_SB SLDI_B3(v16i8, __VA_ARGS__)
SLDI_B3_UH SLDI_B3(v8u16, __VA_ARGS__)
VSHF_B2       \
VSHF_B2_UB VSHF_B2(v16u8, __VA_ARGS__)
VSHF_B2_SB VSHF_B2(v16i8, __VA_ARGS__)
VSHF_B2_UH VSHF_B2(v8u16, __VA_ARGS__)
VSHF_B2_SH VSHF_B2(v8i16, __VA_ARGS__)
VSHF_B3                                          \
VSHF_B3_SB VSHF_B3(v16i8, __VA_ARGS__)
VSHF_B4                            \
VSHF_B4_SB VSHF_B4(v16i8, __VA_ARGS__)
VSHF_B4_SH VSHF_B4(v8i16, __VA_ARGS__)
VSHF_H2       \
VSHF_H2_SH VSHF_H2(v8i16, __VA_ARGS__)
VSHF_H3                                          \
VSHF_H3_SH VSHF_H3(v8i16, __VA_ARGS__)
VSHF_W2      \
VSHF_W2_SB VSHF_W2(v16i8, __VA_ARGS__)
DOTP_UB2   \
DOTP_UB2_UH DOTP_UB2(v8u16, __VA_ARGS__)
DOTP_UB4                      \
DOTP_UB4_UH DOTP_UB4(v8u16, __VA_ARGS__)
DOTP_SB2   \
DOTP_SB2_SH DOTP_SB2(v8i16, __VA_ARGS__)
DOTP_SB3                                 \
DOTP_SB3_SH DOTP_SB3(v8i16, __VA_ARGS__)
DOTP_SB4  \
DOTP_SB4_SH DOTP_SB4(v8i16, __VA_ARGS__)
DOTP_SH2   \
DOTP_SH2_SW DOTP_SH2(v4i32, __VA_ARGS__)
DOTP_SH4                      \
DOTP_SH4_SW DOTP_SH4(v4i32, __VA_ARGS__)
DPADD_SB2   \
DPADD_SB2_SH DPADD_SB2(v8i16, __VA_ARGS__)
DPADD_SB4  \
DPADD_SB4_SH DPADD_SB4(v8i16, __VA_ARGS__)
DPADD_UB2   \
DPADD_UB2_UH DPADD_UB2(v8u16, __VA_ARGS__)
DPADD_SH2   \
DPADD_SH2_SW DPADD_SH2(v4i32, __VA_ARGS__)
DPADD_SH4  \
DPADD_SH4_SW DPADD_SH4(v4i32, __VA_ARGS__)
MIN_UH2               \
MIN_UH2_UH MIN_UH2(v8u16, __VA_ARGS__)
MIN_UH4  \
MIN_UH4_UH MIN_UH4(v8u16, __VA_ARGS__)
CLIP_SH                           \
(  )
CLIP_SH_0_255                                 \
(  )
CLIP_SH2_0_255  \
CLIP_SH4_0_255  \
CLIP_SW_0_255                                 \
(  )
HADD_SW_S32                               \
(  )
HADD_UH_U32                                  \
(  )
HADD_SB2                 \
HADD_SB2_SH HADD_SB2(v8i16, __VA_ARGS__)
HADD_SB4  \
HADD_SB4_UH HADD_SB4(v8u16, __VA_ARGS__)
HADD_SB4_SH HADD_SB4(v8i16, __VA_ARGS__)
HADD_UB2                 \
HADD_UB2_UH HADD_UB2(v8u16, __VA_ARGS__)
HADD_UB3      \
HADD_UB3_UH HADD_UB3(v8u16, __VA_ARGS__)
HADD_UB4  \
HADD_UB4_UB HADD_UB4(v16u8, __VA_ARGS__)
HADD_UB4_UH HADD_UB4(v8u16, __VA_ARGS__)
HADD_UB4_SH HADD_UB4(v8i16, __VA_ARGS__)
HSUB_UB2                 \
HSUB_UB2_UH HSUB_UB2(v8u16, __VA_ARGS__)
HSUB_UB2_SH HSUB_UB2(v8i16, __VA_ARGS__)
HSUB_UB4  \
HSUB_UB4_UH HSUB_UB4(v8u16, __VA_ARGS__)
HSUB_UB4_SH HSUB_UB4(v8i16, __VA_ARGS__)
SAD_UB2_UH                        \
(  )
INSERT_W2                 \
INSERT_W2_UB INSERT_W2(v16u8, __VA_ARGS__)
INSERT_W2_SB INSERT_W2(v16i8, __VA_ARGS__)
INSERT_W4       \
INSERT_W4_UB INSERT_W4(v16u8, __VA_ARGS__)
INSERT_W4_SB INSERT_W4(v16i8, __VA_ARGS__)
INSERT_W4_SW INSERT_W4(v4i32, __VA_ARGS__)
INSERT_D2                 \
INSERT_D2_UB INSERT_D2(v16u8, __VA_ARGS__)
INSERT_D2_SB INSERT_D2(v16i8, __VA_ARGS__)
INSERT_D2_SD INSERT_D2(v2i64, __VA_ARGS__)
ILVEV_B2      \
ILVEV_B2_UB ILVEV_B2(v16u8, __VA_ARGS__)
ILVEV_B2_SB ILVEV_B2(v16i8, __VA_ARGS__)
ILVEV_B2_SH ILVEV_B2(v8i16, __VA_ARGS__)
ILVEV_B2_SD ILVEV_B2(v2i64, __VA_ARGS__)
ILVEV_H2      \
ILVEV_H2_UB ILVEV_H2(v16u8, __VA_ARGS__)
ILVEV_H2_SH ILVEV_H2(v8i16, __VA_ARGS__)
ILVEV_H2_SW ILVEV_H2(v4i32, __VA_ARGS__)
ILVEV_W2      \
ILVEV_W2_UB ILVEV_W2(v16u8, __VA_ARGS__)
ILVEV_W2_SB ILVEV_W2(v16i8, __VA_ARGS__)
ILVEV_W2_UH ILVEV_W2(v8u16, __VA_ARGS__)
ILVEV_W2_SD ILVEV_W2(v2i64, __VA_ARGS__)
ILVEV_D2      \
ILVEV_D2_UB ILVEV_D2(v16u8, __VA_ARGS__)
ILVEV_D2_SB ILVEV_D2(v16i8, __VA_ARGS__)
ILVEV_D2_SW ILVEV_D2(v4i32, __VA_ARGS__)
ILVL_B2      \
ILVL_B2_UB ILVL_B2(v16u8, __VA_ARGS__)
ILVL_B2_SB ILVL_B2(v16i8, __VA_ARGS__)
ILVL_B2_UH ILVL_B2(v8u16, __VA_ARGS__)
ILVL_B2_SH ILVL_B2(v8i16, __VA_ARGS__)
ILVL_B4                         \
ILVL_B4_UB ILVL_B4(v16u8, __VA_ARGS__)
ILVL_B4_SB ILVL_B4(v16i8, __VA_ARGS__)
ILVL_B4_UH ILVL_B4(v8u16, __VA_ARGS__)
ILVL_B4_SH ILVL_B4(v8i16, __VA_ARGS__)
ILVL_H2      \
ILVL_H2_SH ILVL_H2(v8i16, __VA_ARGS__)
ILVL_H2_SW ILVL_H2(v4i32, __VA_ARGS__)
ILVL_H4                         \
ILVL_H4_SH ILVL_H4(v8i16, __VA_ARGS__)
ILVL_H4_SW ILVL_H4(v4i32, __VA_ARGS__)
ILVL_W2      \
ILVL_W2_UB ILVL_W2(v16u8, __VA_ARGS__)
ILVL_W2_SB ILVL_W2(v16i8, __VA_ARGS__)
ILVL_W2_SH ILVL_W2(v8i16, __VA_ARGS__)
ILVR_B2      \
ILVR_B2_UB ILVR_B2(v16u8, __VA_ARGS__)
ILVR_B2_SB ILVR_B2(v16i8, __VA_ARGS__)
ILVR_B2_UH ILVR_B2(v8u16, __VA_ARGS__)
ILVR_B2_SH ILVR_B2(v8i16, __VA_ARGS__)
ILVR_B2_SW ILVR_B2(v4i32, __VA_ARGS__)
ILVR_B3  \
ILVR_B3_UB ILVR_B3(v16u8, __VA_ARGS__)
ILVR_B3_UH ILVR_B3(v8u16, __VA_ARGS__)
ILVR_B3_SH ILVR_B3(v8i16, __VA_ARGS__)
ILVR_B4                         \
ILVR_B4_UB ILVR_B4(v16u8, __VA_ARGS__)
ILVR_B4_SB ILVR_B4(v16i8, __VA_ARGS__)
ILVR_B4_UH ILVR_B4(v8u16, __VA_ARGS__)
ILVR_B4_SH ILVR_B4(v8i16, __VA_ARGS__)
ILVR_B4_SW ILVR_B4(v4i32, __VA_ARGS__)
ILVR_B8   \
ILVR_B8_UH ILVR_B8(v8u16, __VA_ARGS__)
ILVR_H2      \
ILVR_H2_SH ILVR_H2(v8i16, __VA_ARGS__)
ILVR_H2_SW ILVR_H2(v4i32, __VA_ARGS__)
ILVR_H3  \
ILVR_H3_SH ILVR_H3(v8i16, __VA_ARGS__)
ILVR_H4                         \
ILVR_H4_SH ILVR_H4(v8i16, __VA_ARGS__)
ILVR_H4_SW ILVR_H4(v4i32, __VA_ARGS__)
ILVR_W2      \
ILVR_W2_UB ILVR_W2(v16u8, __VA_ARGS__)
ILVR_W2_SB ILVR_W2(v16i8, __VA_ARGS__)
ILVR_W2_SH ILVR_W2(v8i16, __VA_ARGS__)
ILVR_W4                         \
ILVR_W4_SB ILVR_W4(v16i8, __VA_ARGS__)
ILVR_W4_UB ILVR_W4(v16u8, __VA_ARGS__)
ILVR_D2          \
ILVR_D2_UB ILVR_D2(v16u8, __VA_ARGS__)
ILVR_D2_SB ILVR_D2(v16i8, __VA_ARGS__)
ILVR_D2_SH ILVR_D2(v8i16, __VA_ARGS__)
ILVR_D3  \
ILVR_D3_SB ILVR_D3(v16i8, __VA_ARGS__)
ILVR_D4                         \
ILVR_D4_SB ILVR_D4(v16i8, __VA_ARGS__)
ILVR_D4_UB ILVR_D4(v16u8, __VA_ARGS__)
ILVRL_B2               \
ILVRL_B2_UB ILVRL_B2(v16u8, __VA_ARGS__)
ILVRL_B2_SB ILVRL_B2(v16i8, __VA_ARGS__)
ILVRL_B2_UH ILVRL_B2(v8u16, __VA_ARGS__)
ILVRL_B2_SH ILVRL_B2(v8i16, __VA_ARGS__)
ILVRL_B2_SW ILVRL_B2(v4i32, __VA_ARGS__)
ILVRL_H2               \
ILVRL_H2_SB ILVRL_H2(v16i8, __VA_ARGS__)
ILVRL_H2_SH ILVRL_H2(v8i16, __VA_ARGS__)
ILVRL_H2_SW ILVRL_H2(v4i32, __VA_ARGS__)
ILVRL_W2               \
ILVRL_W2_UB ILVRL_W2(v16u8, __VA_ARGS__)
ILVRL_W2_SH ILVRL_W2(v8i16, __VA_ARGS__)
ILVRL_W2_SW ILVRL_W2(v4i32, __VA_ARGS__)
MAXI_SH2                 \
MAXI_SH2_UH MAXI_SH2(v8u16, __VA_ARGS__)
MAXI_SH2_SH MAXI_SH2(v8i16, __VA_ARGS__)
MAXI_SH4  \
MAXI_SH4_UH MAXI_SH4(v8u16, __VA_ARGS__)
SAT_UH2               \
SAT_UH2_UH SAT_UH2(v8u16, __VA_ARGS__)
SAT_UH2_SH SAT_UH2(v8i16, __VA_ARGS__)
SAT_UH4  \
SAT_UH4_UH SAT_UH4(v8u16, __VA_ARGS__)
SAT_SH2               \
SAT_SH2_SH SAT_SH2(v8i16, __VA_ARGS__)
SAT_SH3          \
SAT_SH3_SH SAT_SH3(v8i16, __VA_ARGS__)
SAT_SH4  \
SAT_SH4_SH SAT_SH4(v8i16, __VA_ARGS__)
SAT_SW2               \
SAT_SW2_SW SAT_SW2(v4i32, __VA_ARGS__)
SAT_SW4  \
SAT_SW4_SW SAT_SW4(v4i32, __VA_ARGS__)
SPLATI_H2  \
SPLATI_H2_SB SPLATI_H2(v16i8, __VA_ARGS__)
SPLATI_H2_SH SPLATI_H2(v8i16, __VA_ARGS__)
SPLATI_H3                   \
SPLATI_H3_SB SPLATI_H3(v16i8, __VA_ARGS__)
SPLATI_H3_SH SPLATI_H3(v8i16, __VA_ARGS__)
SPLATI_H4             \
SPLATI_H4_SB SPLATI_H4(v16i8, __VA_ARGS__)
SPLATI_H4_SH SPLATI_H4(v8i16, __VA_ARGS__)
SPLATI_W2            \
SPLATI_W2_SH SPLATI_W2(v8i16, __VA_ARGS__)
SPLATI_W2_SW SPLATI_W2(v4i32, __VA_ARGS__)
SPLATI_W4  \
SPLATI_W4_SH SPLATI_W4(v8i16, __VA_ARGS__)
SPLATI_W4_SW SPLATI_W4(v4i32, __VA_ARGS__)
PCKEV_B2      \
PCKEV_B2_SB PCKEV_B2(v16i8, __VA_ARGS__)
PCKEV_B2_UB PCKEV_B2(v16u8, __VA_ARGS__)
PCKEV_B2_SH PCKEV_B2(v8i16, __VA_ARGS__)
PCKEV_B2_SW PCKEV_B2(v4i32, __VA_ARGS__)
PCKEV_B3  \
PCKEV_B3_UB PCKEV_B3(v16u8, __VA_ARGS__)
PCKEV_B3_SB PCKEV_B3(v16i8, __VA_ARGS__)
PCKEV_B4                         \
PCKEV_B4_SB PCKEV_B4(v16i8, __VA_ARGS__)
PCKEV_B4_UB PCKEV_B4(v16u8, __VA_ARGS__)
PCKEV_B4_SH PCKEV_B4(v8i16, __VA_ARGS__)
PCKEV_B4_SW PCKEV_B4(v4i32, __VA_ARGS__)
PCKEV_H2      \
PCKEV_H2_SH PCKEV_H2(v8i16, __VA_ARGS__)
PCKEV_H2_SW PCKEV_H2(v4i32, __VA_ARGS__)
PCKEV_H4                         \
PCKEV_H4_SH PCKEV_H4(v8i16, __VA_ARGS__)
PCKEV_H4_SW PCKEV_H4(v4i32, __VA_ARGS__)
PCKEV_D2      \
PCKEV_D2_UB PCKEV_D2(v16u8, __VA_ARGS__)
PCKEV_D2_SB PCKEV_D2(v16i8, __VA_ARGS__)
PCKEV_D2_SH PCKEV_D2(v8i16, __VA_ARGS__)
PCKEV_D4                         \
PCKEV_D4_UB PCKEV_D4(v16u8, __VA_ARGS__)
PCKOD_D2      \
PCKOD_D2_UB PCKOD_D2(v16u8, __VA_ARGS__)
PCKOD_D2_SH PCKOD_D2(v8i16, __VA_ARGS__)
PCKOD_D2_SD PCKOD_D2(v2i64, __VA_ARGS__)
XORI_B2_128               \
XORI_B2_128_UB XORI_B2_128(v16u8, __VA_ARGS__)
XORI_B2_128_SB XORI_B2_128(v16i8, __VA_ARGS__)
XORI_B2_128_SH XORI_B2_128(v8i16, __VA_ARGS__)
XORI_B3_128          \
XORI_B3_128_SB XORI_B3_128(v16i8, __VA_ARGS__)
XORI_B4_128  \
XORI_B4_128_UB XORI_B4_128(v16u8, __VA_ARGS__)
XORI_B4_128_SB XORI_B4_128(v16i8, __VA_ARGS__)
XORI_B4_128_SH XORI_B4_128(v8i16, __VA_ARGS__)
XORI_B5_128  \
XORI_B5_128_SB XORI_B5_128(v16i8, __VA_ARGS__)
XORI_B6_128  \
XORI_B6_128_SB XORI_B6_128(v16i8, __VA_ARGS__)
XORI_B7_128  \
XORI_B7_128_SB XORI_B7_128(v16i8, __VA_ARGS__)
XORI_B8_128  \
XORI_B8_128_SB XORI_B8_128(v16i8, __VA_ARGS__)
ADDS_SH2       \
ADDS_SH2_SH ADDS_SH2(v8i16, __VA_ARGS__)
ADDS_SH4                         \
ADDS_SH4_UH ADDS_SH4(v8u16, __VA_ARGS__)
ADDS_SH4_SH ADDS_SH4(v8i16, __VA_ARGS__)
SLLI_4V  \
SRA_4V  \
SRL_H4            \
SRL_H4_UH SRL_H4(v8u16, __VA_ARGS__)
SRAR_H2                      \
SRAR_H2_UH SRAR_H2(v8u16, __VA_ARGS__)
SRAR_H2_SH SRAR_H2(v8i16, __VA_ARGS__)
SRAR_H3                 \
SRAR_H3_SH SRAR_H3(v8i16, __VA_ARGS__)
SRAR_H4  \
SRAR_H4_UH SRAR_H4(v8u16, __VA_ARGS__)
SRAR_H4_SH SRAR_H4(v8i16, __VA_ARGS__)
SRAR_W2                      \
SRAR_W2_SW SRAR_W2(v4i32, __VA_ARGS__)
SRAR_W4  \
SRAR_W4_SW SRAR_W4(v4i32, __VA_ARGS__)
SRARI_H2              \
SRARI_H2_UH SRARI_H2(v8u16, __VA_ARGS__)
SRARI_H2_SH SRARI_H2(v8i16, __VA_ARGS__)
SRARI_H4    \
SRARI_H4_UH SRARI_H4(v8u16, __VA_ARGS__)
SRARI_H4_SH SRARI_H4(v8i16, __VA_ARGS__)
SRARI_W2              \
SRARI_W2_SW SRARI_W2(v4i32, __VA_ARGS__)
SRARI_W4  \
SRARI_W4_SH SRARI_W4(v8i16, __VA_ARGS__)
SRARI_W4_SW SRARI_W4(v4i32, __VA_ARGS__)
MUL2  \
MUL4  \
ADD2  \
ADD4  \
SUB2  \
SUB4  \
UNPCK_R_SH_SW                       \
UNPCK_SB_SH                  \
UNPCK_UB_SH                   \
UNPCK_SH_SW                  \
SWAP  \
BUTTERFLY_4  \
BUTTERFLY_8  \
BUTTERFLY_16  \
TRANSPOSE4x4_UB_UB  \
TRANSPOSE8x4_UB                         \
TRANSPOSE8x4_UB_UB TRANSPOSE8x4_UB(v16u8, __VA_ARGS__)
TRANSPOSE8x4_UB_UH TRANSPOSE8x4_UB(v8u16, __VA_ARGS__)
TRANSPOSE8x8_UB  \
TRANSPOSE8x8_UB_UB TRANSPOSE8x8_UB(v16u8, __VA_ARGS__)
TRANSPOSE8x8_UB_UH TRANSPOSE8x8_UB(v8u16, __VA_ARGS__)
TRANSPOSE16x4_UB_UB                        \
TRANSPOSE16x8_UB_UB  \
TRANSPOSE4x4_SH_SH  \
TRANSPOSE8x8_H  \
TRANSPOSE8x8_UH_UH TRANSPOSE8x8_H(v8u16, __VA_ARGS__)
TRANSPOSE8x8_SH_SH TRANSPOSE8x8_H(v8i16, __VA_ARGS__)
TRANSPOSE4x4_SW_SW  \
AVE_ST8x4_UB  \
AVE_ST16x4_UB  \
AVER_ST8x4_UB  \
AVER_ST16x4_UB  \
AVER_DST_ST8x4_UB                            \
AVER_DST_ST16x4_UB                            \
ADDBLK_ST4x4_UB         \
DPADD_SH3_SH         \
(  )
PCKEV_XORI128_UB                            \
(  )
CONVERT_UB_AVG_ST8x4_UB  \
PCKEV_ST4x4_UB  \
PCKEV_ST_SB                   \
HORIZ_2TAP_FILT_UH            \
(  )
